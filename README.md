# Faster R-CNN: Two-Stage Object Detection from First Principles

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org)
[![Paper](https://img.shields.io/badge/Paper-NeurIPS_2015-blue)](https://arxiv.org/abs/1506.01497)
[![Implementation](https://img.shields.io/badge/Implementation-From_Scratch-orange)]()

A **from-scratch** PyTorch implementation of Faster R-CNN, designed to deeply understand the mathematics and engineering behind two-stage object detection.

> **Original Paper**: [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) (Ren et al., NeurIPS 2015)

---

## ğŸ”¬ Research Motivation

**The Evolution:**
- **R-CNN** (2014): Extract ~2000 region proposals with Selective Search â†’ CNN features â†’ SVM classifier. **Bottleneck**: Selective Search is slow.
- **Fast R-CNN** (2015): Share CNN computation across proposals. **Bottleneck**: Still uses Selective Search.
- **Faster R-CNN** (2015): Replace Selective Search with a learned **Region Proposal Network (RPN)**. **Result**: End-to-end trainable, 250ms/image.

**Key Insight:** Region proposals can be learned from the same CNN features used for classification, making detection fully differentiable.

---

## ğŸ§® Mathematical Formulations

### Anchor Box Generation

At each feature map location $(x, y)$, generate $k$ anchor boxes with:
- **Scales**: $s \in \{2, 4, 6\}$ (relative to feature map)
- **Aspect Ratios**: $r \in \{0.5, 1.0, 1.5\}$

Each anchor is defined as:

$$\text{anchor} = \left(x - \frac{w}{2}, y - \frac{h}{2}, x + \frac{w}{2}, y + \frac{h}{2}\right)$$

where $w = s \cdot r$ and $h = s$.

**Total anchors**: For a $20 \times 15$ feature map with 9 anchors each = **2,700 proposals/image**.

### Intersection over Union (IoU)

$$\text{IoU}(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{\text{Intersection Area}}{\text{Union Area}}$$

**Anchor assignment:**
- **Positive**: IoU > 0.7 with any GT box, OR highest IoU for a GT box
- **Negative**: IoU < 0.3 with all GT boxes
- **Ignore**: 0.3 â‰¤ IoU â‰¤ 0.7 (not used in training)

### Bounding Box Regression

Transform from anchor $A = (A_x, A_y, A_w, A_h)$ to ground truth $G = (G_x, G_y, G_w, G_h)$:

$$t_x = \frac{G_x - A_x}{A_w}, \quad t_y = \frac{G_y - A_y}{A_h}$$

$$t_w = \log\left(\frac{G_w}{A_w}\right), \quad t_h = \log\left(\frac{G_h}{A_h}\right)$$

**Why log for width/height?** Ensures predictions are always positive and handles large scale variations.

**Inverse transform (inference):**

$$P_x = A_x + t_x \cdot A_w, \quad P_y = A_y + t_y \cdot A_h$$

$$P_w = A_w \cdot e^{t_w}, \quad P_h = A_h \cdot e^{t_h}$$

### Loss Functions

**RPN Classification Loss (Binary Cross-Entropy):**

$$\mathcal{L}_{cls} = -\frac{1}{N_{cls}} \sum_i \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]$$

**RPN Regression Loss (Smooth L1):**

$$\mathcal{L}_{reg} = \frac{1}{N_{reg}} \sum_i \text{smooth}_{L1}(t_i - t_i^*)$$

where:

$$\text{smooth}_{L1}(x) = \begin{cases} 
0.5x^2 & \text{if } |x| < 1 \\
|x| - 0.5 & \text{otherwise}
\end{cases}$$

**Why Smooth L1?** More robust to outliers than L2, while having stable gradients near zero.

**Total Loss:**

$$\mathcal{L} = \mathcal{L}_{cls} + \lambda \mathcal{L}_{reg}$$

where $\lambda = 5$ balances the two losses.

### Non-Maximum Suppression (NMS)

```
1. Sort proposals by confidence score
2. Select highest-scoring box, add to output
3. Remove all boxes with IoU > threshold (0.7) with selected box
4. Repeat until no boxes remain
```

---

## ğŸ—ï¸ Architecture

```
Input Image (HÃ—WÃ—3)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ResNet-50 Backbone (layers 1-4)               â”‚
â”‚   Output: (B, 2048, H/32, W/32)                 â”‚
â”‚   Purpose: Extract rich semantic features       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Region Proposal Network (RPN)                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ 3Ã—3 Conv (512 channels)                 â”‚   â”‚
â”‚   â”‚ â”œâ”€â”€ Objectness Head: 1Ã—1 Conv â†’ A       â”‚   â”‚
â”‚   â”‚ â”‚   (Is there an object?)               â”‚   â”‚
â”‚   â”‚ â””â”€â”€ Regression Head: 1Ã—1 Conv â†’ 4A      â”‚   â”‚
â”‚   â”‚     (Box offsets: tx, ty, tw, th)       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   â†“                                             â”‚
â”‚   Generate proposals + NMS â†’ Top-N boxes        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Proposals (~300 boxes)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ROI Pooling                                   â”‚
â”‚   Extract fixed 7Ã—7 features for each proposal  â”‚
â”‚   (Handles variable-sized boxes)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Classification Head                           â”‚
â”‚   AvgPool â†’ FC(512) â†’ FC(N_classes)            â”‚
â”‚   Output: Class probabilities                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Implementation Details

### Core Components

| Module | Parameters | Key Design Choices |
|--------|------------|-------------------|
| `FeatureExtractor` | ResNet-50 (23M) | Layers 1-4, all trainable |
| `ProposalModule` | ~2.4M | 512 hidden dim, dropout=0.3 |
| `ClassificationModule` | ~1.1M | 7Ã—7 ROI, avg pool |

### Anchor Box Visualization

```
Feature Map Position (x, y):

         r=0.5    r=1.0    r=1.5
        â”Œâ”€â”      â”Œâ”€â”€â”     â”Œâ”€â”€â”€â”
s=2     â”‚ â”‚      â”‚  â”‚     â”‚   â”‚
        â””â”€â”˜      â””â”€â”€â”˜     â””â”€â”€â”€â”˜

        â”Œâ”€â”€â”     â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”
s=4     â”‚  â”‚     â”‚    â”‚   â”‚     â”‚
        â””â”€â”€â”˜     â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
s=6     â”‚   â”‚    â”‚      â”‚ â”‚       â”‚
        â””â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory-Efficient IoU Computation

```python
# Vectorized IoU for (N anchors Ã— M GT boxes)
def get_iou_mat(anc_boxes, gt_bboxes):
    # anc_boxes: (N, 4), gt_bboxes: (M, 4)
    # Returns: (N, M) IoU matrix
    return ops.box_iou(anc_boxes, gt_bboxes)  # O(NM) but vectorized
```

---

## ğŸ’¡ Insights from Implementation

### What I Learned

1. **Anchor assignment is critical**: The IoU thresholds (0.7/0.3) create a "gray zone" that prevents noisy gradients from ambiguous boxes.

2. **Balanced sampling matters**: Negative anchors vastly outnumber positives (~100:1). Random sampling of negatives to match positives prevents the model from predicting "no object" everywhere.

3. **Spatial scale alignment**: `roi_pool` requires `spatial_scale=1/32` to match ResNet's downsampling factor. Wrong scale = garbage features.

4. **Gradient flow through proposals**: During training, proposals are detached (`proposals.detach()`) to prevent gradients from flowing back through NMS (non-differentiable).

### Challenges Encountered

| Challenge | Solution |
|-----------|----------|
| Memory explosion with all anchors | Sample 256 anchors/image (128 pos + 128 neg) |
| NMS is non-differentiable | Detach proposals before classification |
| Coordinate system confusion | Consistent use of `xyxy` format, project with scale factors |
| Class imbalance | Balanced sampling + focal loss (future work) |

### Debugging Tips

```python
# Sanity check: visualize anchors
fig, ax = plt.subplots()
display_bbox(anchors[:100], fig, ax, color='blue')  # Sample anchors
display_bbox(gt_boxes, fig, ax, color='red')        # Ground truth
```

---

## ğŸ”¬ Ablation Study Design

### Planned Experiments

| Experiment | Variable | Expected Result |
|------------|----------|-----------------|
| **Anchor scales** | {64, 128, 256} vs {2, 4, 6} | Larger scales for larger objects |
| **IoU thresholds** | 0.5/0.1 vs 0.7/0.3 | Lower thresholds â†’ more positives but noisier |
| **Loss weight Î»** | 1 vs 5 vs 10 | Higher Î» â†’ tighter boxes but slower convergence |
| **NMS threshold** | 0.5 vs 0.7 vs 0.9 | Lower â†’ fewer proposals, higher precision |
| **ROI size** | 5Ã—5 vs 7Ã—7 vs 14Ã—14 | Larger â†’ more detail but slower |

### Baseline Comparisons

| Method | mAP (VOC) | Speed |
|--------|-----------|-------|
| Selective Search + CNN | 58.5% | ~47s/image |
| Fast R-CNN | 66.9% | ~2s/image |
| **Faster R-CNN** | **69.9%** | **0.2s/image** |

---

## ğŸ“Š Training Pipeline

```
Epoch Loop:
â”‚
â”œâ”€â”€ For each batch:
â”‚   â”œâ”€â”€ Extract features (backbone)
â”‚   â”œâ”€â”€ Generate anchors (grid Ã— scales Ã— ratios)
â”‚   â”œâ”€â”€ Compute IoU matrix (anchors Ã— GT)
â”‚   â”œâ”€â”€ Assign labels (pos/neg/ignore)
â”‚   â”œâ”€â”€ Sample 256 anchors (balanced)
â”‚   â”œâ”€â”€ RPN forward â†’ conf scores, offsets
â”‚   â”œâ”€â”€ RPN loss = BCE + Smooth L1
â”‚   â”œâ”€â”€ Generate proposals (anchor + offset)
â”‚   â”œâ”€â”€ NMS â†’ top-N proposals
â”‚   â”œâ”€â”€ ROI pooling â†’ fixed-size features
â”‚   â”œâ”€â”€ Classification head â†’ class scores
â”‚   â”œâ”€â”€ Classification loss = CrossEntropy
â”‚   â””â”€â”€ Total loss = RPN loss + Cls loss
â”‚
â””â”€â”€ Backprop and optimize
```

---

## ğŸ”§ Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| `pos_thresh` | 0.7 | Standard from paper |
| `neg_thresh` | 0.3 | Creates clear separation |
| `Î» (loss weight)` | 5 | Regression needs more weight |
| `n_anchors` | 9 | 3 scales Ã— 3 ratios |
| `nms_thresh` | 0.7 | Balances recall/precision |
| `conf_thresh` | 0.5 | Filter low-confidence proposals |

---

## ğŸ“š Citation

```bibtex
@inproceedings{ren2015faster,
  title={Faster {R-CNN}: Towards Real-Time Object Detection 
         with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={NeurIPS},
  pages={91--99},
  year={2015}
}
```

---

## ğŸ”® Future Directions

1. **Feature Pyramid Network (FPN)**: Multi-scale detection for small objects
2. **Focal Loss**: Address class imbalance more elegantly
3. **Deformable Convolutions**: Better handle geometric variations
4. **Cascade R-CNN**: Progressive refinement for tighter boxes
5. **ROI Align**: Replace ROI Pool for pixel-perfect alignment

---

## ğŸ“„ License

MIT License
